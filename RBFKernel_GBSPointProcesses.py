#!/usr/bin/env python
# coding: utf-8

# # StrawberryFields code on Point Processes with RBF kernel, GBS Sampling and exploration of the method with Higher dimensional Stock Data.
# 
# This code will review the exploration discussed by Jahangiri et al. (2020) in their paper on Point Processes with Gaussian Boson Sampling [^1]. It will provide the main code provided in StrawberryFields for most applications, which will hopefully help in future explorations.
# 
# ## References
# 
# [^1]: Jahangiri, S., Arrazola, J. M., Quesada, N., & Killoran, N. (2020). Point Processes With Gaussian Boson Sampling. *Physical Review E*, 101, 022134. https://doi.org/10.1103/physreve.101.022134

# In[12]:


# First exploration will be an RBF kernel in homogeneous space (not applied to real world data yet)
# This is from StrawberryFields

import numpy as np
import plotly
from sklearn.datasets import make_blobs
from strawberryfields.apps import points, plot
import pandas as pd
import seaborn as sns
import zipfile
import matplotlib.pyplot as plt
from scipy.optimize import root_scalar
import random
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from strawberryfields.apps import sample


# Define the space for the GBS point process patterns
# This space is referred to as the state space and is defined by a set of points.
# In this case, we are creating a 20x20 square grid of points.
# Each row of R contains the coordinates of one point in the grid.
R = np.array([(i, j) for i in range(20) for j in range(20)])

# Next, we create the kernel matrix for these points using the radial basis function (RBF) kernel.
# The RBF kernel defines the similarity between each pair of points (i, j) as:
# K(i,j) = exp(-||r_i - r_j||^2 / (2 * sigma^2))
# Here, 'sigma' is a kernel parameter that determines the scale of the kernel.
# Points that are much closer than 'sigma' have larger kernel entries, while distant points have smaller entries.
sigma = 2.5
K = points.rbf_kernel(R, sigma)

# Generate samples using the GBS point process
# We generate 10 samples, with an average of 50 points selected in each sample.
# The point process will select subsets of the points in R based on the structure of the kernel matrix K.
# Points that are close to each other (i.e., have high kernel matrix values) will have a higher chance of being selected together.
num_samples = 10
avg_num_points_per_sample = 50.0
samples = points.sample(K, avg_num_points_per_sample, num_samples)

# Now we visualize the point patterns generated by the permanental point process.
# The point patterns tend to exhibit a higher degree of clustering compared to a uniformly random pattern.
# We use the plot module to visualize the first sample generated by the GBS point process.
plot.points(R, samples[0], point_size=10)

# Note:
# 1. The 'samples' array contains the sampled points for each of the 'num_samples' samples.
# 2. Each entry in the samples matrix is binary, indicating whether a point in R was selected in that particular sample.
# 3. The 'plot.points()' function visualizes the selected points, where more frequently sampled points tend to cluster together.


# In[13]:


# Second exploration is the same but in aan inhomogeneous state space (not applied to real wrld data) fro Strawberryfiedls to detect outliers
# Outlier Detection Using GBS Point Processes

# Step 1: Create data points with two dense clusters and some randomly distributed points (background noise).
# The clusters have 50 points each, with a standard deviation of 0.3.
# The clusters are centered at (2,2) and (4,4).
clusters = make_blobs(n_samples=100, centers=[[2, 2], [4, 4]], cluster_std=0.3)[0]

# Add 25 random background noise points, uniformly distributed across a 6x6 space.
# These points represent the "outliers" in this dataset.
noise = np.random.rand(25, 2) * 6.0

# Combine the clustered points and the noise points into a single dataset.
R = np.concatenate((clusters, noise))

# Step 2: Construct the kernel matrix using the Radial Basis Function (RBF) kernel.
# The RBF kernel defines similarity between points based on their distances.
# Sigma is set to 1.0, controlling the scale of the kernel (how sensitive it is to distances between points).
K = points.rbf_kernel(R, 1.0)

# Step 3: Generate GBS point process samples.
# We generate 10,000 samples, with an average of 10 points selected in each sample.
# The GBS point process selects points from dense regions (clusters) with a higher probability.
num_samples = 10000
avg_num_points_per_sample = 10.0
samples = points.sample(K, avg_num_points_per_sample, num_samples)

# Step 4: Find the indices of the 100 points that appear most frequently in the GBS samples.
# Points from the dense clusters will tend to appear more frequently in the samples,
# while the background points (outliers) will appear less frequently.
gbs_frequent_points = np.argsort(np.sum(samples, axis=0))[-100:]

# Step 5: Visualize the points.
# Points that are frequently selected by the GBS process (i.e., belong to the dense clusters) will be highlighted.
# Outliers will not be highlighted as they are selected with lower frequency.
plot.points(
    R, [1 if i in gbs_frequent_points else 0 for i in range(len(samples[0]))], point_size=10
)

# Explanation:
# - 'samples' contains binary arrays indicating which points were selected in each of the 10,000 samples.
# - 'np.sum(samples, axis=0)' sums over the number of times each point is selected across all samples.
# - 'np.argsort()[-100:]' retrieves the indices of the 100 most frequently selected points.
# - 'plot.points()' visualizes these points, highlighting the clusters and identifying outliers.

# Note: The points most frequently selected will be from the two clusters centered at (2,2) and (4,4),
# while the randomly distributed noise points (outliers) will appear less frequently.


# # In strawberryfields they create an RBF kernel matrix from points in a Euclidean space (2D) but may be extended to higher dimensions to include real world data
# 
# Now lets explore how to construct this RBF kernel using stock data. From here every line of code has been created by me. The goal is to apply this modelue to stock data in higher dimension s and see if this kernel matrix offers an advantage against the stock correlation matrix explored in [^1]

# In[84]:


# Function to extract closing prices for a specific stock
def extract_closing_prices(stock_ticker, df, start_date=None, end_date=None):
    if start_date and end_date:
        return df.loc[start_date:end_date, stock_ticker]
    elif start_date:
        return df.loc[start_date:, stock_ticker]
    elif end_date:
        return df.loc[:end_date, stock_ticker]
    else:
        return df[stock_ticker]

# Function to compute daily returns
def compute_daily_returns(df):
    daily_returns = df.pct_change().dropna()
    return daily_returns

#Function to compute log returns

def compute_log_returns(df):
    daily_returns = df.pct_change().dropna()
    log_returns = np.log(1 + daily_returns)
    return log_returns

# Function to compute correlation matrix jahanguiri
    
def compute_correlation_matrix_J(returns):
    n_days = len(returns)
    correlation_matrix_0 = sum(np.outer(returns.iloc[i], returns.iloc[i]) for i in range(n_days)) / n_days
    rescaling_factor = n_days
    correlation_matrix = correlation_matrix_0 * rescaling_factor
    #correlation_matrix = correlation_matrix + 1j * np.zeros_like(correlation_matrix)
    return correlation_matrix
    
    
 #Function to compute co-variance matrix from Python function

def compute_covariance_matrix(returns):
    n_days = len(returns)
    covariance_matrix  = np.cov(returns, rowvar=False)
    return covariance_matrix

#Compute Correlation matrix according to Python

def correlation_matrix (returns):
    correlation_matrix = daily_returns.corr()
    return correlation_matrix

# Function to perform Takagi-Autonne decomposition

def takagi_autonne_decomposition(A):
    A = (A + A.T.conj()) / 2
    U, s, _ = np.linalg.svd(A)
    lambdas = s
    return lambdas, U

# Function to solve for the constant c
def solve_for_c(lambdas, target_n_mean):
    def mean_photon_number(c):
        return np.sum((c * lambdas)**2 / (1 - (c * lambdas)**2))
    
    result = root_scalar(lambda c: mean_photon_number(c) - target_n_mean, bracket=[0, 1 / np.max(lambdas) - 1e-6])
    return result.root

# Function to calculate squeezing parameters
def calculate_squeezing_parameters(lambdas, c):
    squeezing_parameters = np.arctanh(c * lambdas)
    return squeezing_parameters

# Function to plot a heatmap
def plot_heatmap(matrix, title, ax=None, vmin=None, vmax=None):
    matrix_magnitude = np.abs(matrix)
    if ax is None:
        plt.figure(figsize=(10, 8))
        sns.heatmap(matrix_magnitude, annot=True, cmap="inferno", vmin=vmin, vmax=vmax)
        plt.title(title)
        plt.show()
    else:
        sns.heatmap(matrix_magnitude, annot=True, cmap="inferno", ax=ax, vmin=vmin, vmax=vmax)
        ax.set_title(title)


# In[16]:


# Define the RBF kernel function instead of the 2D one, this uses real-world data inputs instead of coordinates
def rbf_kernel(log_returns, sigma):
    n_stocks = log_returns.shape[1]
    K = np.zeros((n_stocks, n_stocks))
    
    # Compute the RBF kernel matrix
    for i in range(n_stocks):
        for j in range(n_stocks):
            diff = log_returns.iloc[:, i] - log_returns.iloc[:, j]
            K[i, j] = np.exp(-np.sum(diff**2) / (2 * sigma ** 2))
    
    return K


# In[77]:


# Load the data
zip_file_path = r'C:\Users\user\Downloads\archive (1).zip'
csv_file_name = 'all_stocks_5yr.csv'
with zipfile.ZipFile(zip_file_path, 'r') as z:
    with z.open(csv_file_name) as f:
        all_stocks_df = pd.read_csv(f)

# Choose subsets of stocks
selected_stocks_diff_sector = ["AAPL", "JPM", "XOM", "JNJ", "DIS", "CAT", "PG", "NKE", "CLX"]
selected_stocks_same_sector = ["MRO", "DVN", "COP", "KMI", "COG", "PSX", "CVX", "OXY", "PXD"] 

# Filter the dataset for the selected stocks
filtered_df_diff_sector = all_stocks_df[all_stocks_df['Name'].isin(selected_stocks_diff_sector)]
filtered_df_same_sector = all_stocks_df[all_stocks_df['Name'].isin(selected_stocks_same_sector)]

# Pivot the data to have stock names as columns and dates as rows
pivot_df_diff_sector = filtered_df_diff_sector.pivot(index='date', columns='Name', values='close')
pivot_df_same_sector = filtered_df_same_sector.pivot(index='date', columns='Name', values='close')

# Compute log returns
log_returns_diff_sector = compute_log_returns(pivot_df_diff_sector)
log_returns_same_sector = compute_log_returns(pivot_df_same_sector)

sigma = 15

# Compute the RBF kernel matrices for different and same sector stocks
K_diff_sector = rbf_kernel(log_returns_diff_sector, sigma)
K_same_sector = rbf_kernel(log_returns_same_sector, sigma)

# Plot the heat maps for both sectors
plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.heatmap(K_diff_sector, xticklabels=selected_stocks_diff_sector, yticklabels=selected_stocks_diff_sector, cmap='viridis')
plt.title('RBF Kernel Matrix (Different Sectors)')

plt.subplot(1, 2, 2)
sns.heatmap(K_same_sector, xticklabels=selected_stocks_same_sector, yticklabels=selected_stocks_same_sector, cmap='viridis')
plt.title('RBF Kernel Matrix (Same Sector)')

plt.tight_layout()
plt.show()


# In[26]:


samples_diff = points.sample(K_diff_sector, 50.0, 10)
samples_same = points.sample(K_same_sector, 50.0, 10)


# In[28]:


# Sum over all samples to get the total number of times each stock is selected
sampling_frequencies = np.sum(samples_diff, axis=0)

# Create a bar plot to show the sampling frequencies for each stock
plt.figure(figsize=(10, 6))
plt.bar(log_returns_diff_sector.columns, sampling_frequencies, color='skyblue')
plt.xlabel('Stocks')
plt.ylabel('Sampling Frequency')
plt.title('Sampling Frequencies of Stocks in GBS Point Process (different sectors)')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


# In[29]:


# Sum over all samples to get the total number of times each stock is selected
sampling_frequencies = np.sum(samples_same, axis=0)

# Create a bar plot to show the sampling frequencies for each stock
plt.figure(figsize=(10, 6))
plt.bar(log_returns_same_sector.columns, sampling_frequencies, color='skyblue')
plt.xlabel('Stocks')
plt.ylabel('Sampling Frequency')
plt.title('Sampling Frequencies of Stocks in GBS Point Process (same sectors)')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


# In[32]:


# Select a specific sample to visualize photon counts
sample_index = 0  # Index of the sample you want to print
single_sample = samples_same[sample_index]

# Create a bar plot to show the photon counts for the selected sample
plt.figure(figsize=(10, 6))
plt.bar(log_returns_same_sector.columns, single_sample, color='skyblue')
plt.xlabel('Stocks')
plt.ylabel('Photon Count (Sample 1 PNR detectors)')
plt.title(f'Photon Counts (Selection) for Sample {sample_index + 1}')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


# # Now sample using GBS PP

# In[78]:


# Generate samples
n_mean = 18
samples = 300

s_thresh_diff = sample.sample(K_diff_sector, n_mean, samples, threshold=True, loss=0.0)
s_thresh_same = sample.sample(K_same_sector , n_mean, samples, threshold=True, loss=0.0)
s_pnr_diff = sample.sample(K_diff_sector, n_mean, samples, threshold=False, loss=0.0)
s_pnr_same = sample.sample(K_same_sector, n_mean, samples, threshold=False, loss=0.0)

# Convert the lists to NumPy arrays
s_thresh_diff = np.array(s_thresh_diff)
s_thresh_same = np.array(s_thresh_same)
s_pnr_diff = np.array(s_pnr_diff)
s_pnr_same = np.array(s_pnr_same)


# In[79]:


# Check which threshold samples for different sectors are empty, there seems to be a few, would be good to understand why
empty_thresh_diff = np.where(~s_thresh_diff.any(axis=1))[0]
empty_thresh_same = np.where(~s_thresh_same.any(axis=1))[0]

# Check which PNR samples for different sectors are empty
empty_pnr_diff = np.where(~s_pnr_diff.any(axis=1))[0]
empty_pnr_same = np.where(~s_pnr_same.any(axis=1))[0]

print(f"Empty Threshold Samples (Different Sectors): {empty_thresh_diff}")
print(f"Empty Threshold Samples (Same Sectors): {empty_thresh_same}")
print(f"Empty PNR Samples (Different Sectors): {empty_pnr_diff}")
print(f"Empty PNR Samples (Same Sectors): {empty_pnr_same}")


# In[81]:


plt.figure(figsize=(16, 12))

# Plot the first threshold sample for different sectors
plt.subplot(2, 2, 1)
plt.bar(stocks_diff_sector, s_thresh_diff[1], color='skyblue')
plt.xlabel('Stocks')
plt.ylabel('Photon Counts')
plt.title('First Threshold Sample (Different Sectors)')
plt.xticks(rotation=90)

# Plot the first threshold sample for same sectors
plt.subplot(2, 2, 2)
plt.bar(stocks_same_sector, s_thresh_same[1], color='lightgreen')
plt.xlabel('Stocks')
plt.ylabel('Photon Counts')
plt.title('First Threshold Sample (Same Sectors)')
plt.xticks(rotation=90)

# Plot the first PNR sample for different sectors
plt.subplot(2, 2, 3)
plt.bar(stocks_diff_sector, s_pnr_diff[1], color='orange')
plt.xlabel('Stocks')
plt.ylabel('Photon Counts')
plt.title('First PNR Sample (Different Sectors)')
plt.xticks(rotation=90)

# Plot the first PNR sample for same sectors
plt.subplot(2, 2, 4)
plt.bar(stocks_same_sector, s_pnr_same[1], color='coral')
plt.xlabel('Stocks')
plt.ylabel('Photon Counts')
plt.title('First PNR Sample (Same Sectors)')
plt.xticks(rotation=90)

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()


# # Create probability distribution

# In[82]:


# Convert the lists to NumPy arrays
s_thresh_diff = np.array(s_thresh_diff)
s_thresh_same = np.array(s_thresh_same)
s_pnr_diff = np.array(s_pnr_diff)
s_pnr_same = np.array(s_pnr_same)

# Calculate the summed and normalized probabilities for PNR
sum_pnr_diff = np.sum(s_pnr_diff, axis=0)
sum_pnr_same = np.sum(s_pnr_same, axis=0)

# Find the maximum count for normalization
max_count_pnr = max(sum_pnr_diff.max(), sum_pnr_same.max())

# Normalize the summed probabilities
s_pnr_diff_norm = sum_pnr_diff / max_count_pnr
s_pnr_same_norm = sum_pnr_same / max_count_pnr

# Calculate the summed and normalized probabilities for Thresh
sum_thresh_diff = np.sum(s_thresh_diff, axis=0)
sum_thresh_same = np.sum(s_thresh_same, axis=0)

# Normalize the summed probabilities
s_thresh_diff_norm = sum_thresh_diff / sum_thresh_diff.max()
s_thresh_same_norm = sum_thresh_same / sum_thresh_same.max()

# Find the maximum count for normalization
max_count_thres = max(sum_thresh_diff.max(), sum_thresh_same.max())


# In[83]:


# Create the figure with subplots (3 rows and 2 columns)
fig, axes = plt.subplots(3, 2, figsize=(20, 24))

# Plot the kernel matrices
sns.heatmap(K_diff_sector, annot=False, cmap="inferno", vmin=0.99, vmax=1, 
            xticklabels=selected_stocks_diff_sector, yticklabels=selected_stocks_diff_sector, ax=axes[0, 0])
axes[0, 0].set_title("Kernel Matrix - GBS (Technology Sectors)")

sns.heatmap(K_same_sector, annot=False, cmap="inferno", vmin=0.99, vmax=1, 
            xticklabels=selected_stocks_same_sector, yticklabels=selected_stocks_same_sector, ax=axes[0, 1])
axes[0, 1].set_title("Kernel Matrix - GBS (Energy Sector)")

# Calculate the summed and normalized probabilities
sum_thresh_diff = np.sum(s_thresh_diff, axis=0)
sum_thresh_same = np.sum(s_thresh_same, axis=0)

# Normalize the summed probabilities
s_thresh_diff_norm = sum_thresh_diff / sum_thresh_diff.max()
s_thresh_same_norm = sum_thresh_same / sum_thresh_same.max()

# Find the maximum count for normalization
max_count_thres = max(sum_thresh_diff.max(), sum_thresh_same.max())

# Plot the normalized probability distribution for the different sectors
axes[1, 0].bar(range(len(s_thresh_diff_norm)), s_thresh_diff_norm, color='skyblue')
axes[1, 0].set_xticks(range(len(selected_stocks_diff_sector)))
axes[1, 0].set_xticklabels(selected_stocks_diff_sector, rotation=90)
axes[1, 0].set_xlabel('Mode')
axes[1, 0].set_ylabel('Normalized Probability')
axes[1, 0].set_ylim(0, 1.1)  # Normalized probabilities should be between 0 and 1
axes[1, 0].set_title('Threshold detector Normalized Probability Distribution from GBS (Different Sector)')

# Plot the normalized probability distribution for the same sector
axes[1, 1].bar(range(len(s_thresh_same_norm)), s_thresh_same_norm, color='skyblue')
axes[1, 1].set_xticks(range(len(selected_stocks_same_sector)))
axes[1, 1].set_xticklabels(selected_stocks_same_sector, rotation=90)
axes[1, 1].set_xlabel('Mode')
axes[1, 1].set_ylabel('Normalized Probability')
axes[1, 1].set_ylim(0, 1.1)  # Normalized probabilities should be between 0 and 1
axes[1, 1].set_title('Threshold detector Normalized Probability Distribution from GBS (Same Sector)')

# Plot the unnormalized summed probabilities for the different sectors
axes[2, 0].bar(range(len(sum_thresh_diff)), sum_thresh_diff, color='orange')
axes[2, 0].set_xticks(range(len(selected_stocks_diff_sector)))
axes[2, 0].set_xticklabels(selected_stocks_diff_sector, rotation=90)
axes[2, 0].set_xlabel('Mode')
axes[2, 0].set_ylabel('Summed Probability')
axes[2, 0].set_ylim(0, max_count_thres) 
axes[2, 0].set_title('Threshold detector Unnormalized Summed Probability Distribution from GBS (Different Sector)')

# Plot the unnormalized summed probabilities for the same sector
axes[2, 1].bar(range(len(sum_thresh_same)), sum_thresh_same, color='orange')
axes[2, 1].set_xticks(range(len(selected_stocks_same_sector)))
axes[2, 1].set_xticklabels(selected_stocks_same_sector, rotation=90)
axes[2, 1].set_xlabel('Mode')
axes[2, 1].set_ylabel('Summed Probability')
axes[2, 1].set_ylim(0, max_count_thres) 
axes[2, 1].set_title('Threshold detector Unnormalized Summed Probability Distribution from GBS (Same Sector)')

plt.tight_layout()

# Save the combined figure as a PDF
plt.savefig(r"C:\Users\user\Downloads\norm&unnorm_probability_distribution_Thresh_EvsT.pdf", format='pdf', dpi=300)

# Now show the plot
plt.show()


# In[55]:


# This shows that probability distributions are not very conclusive but may help the reader in further explorations


# In[ ]:




